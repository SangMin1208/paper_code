[
  {
    "objectID": "posts/02_Generative Adversarial Nets.html",
    "href": "posts/02_Generative Adversarial Nets.html",
    "title": "2. Generative Adversarial Network(GAN))",
    "section": "",
    "text": "title : Generative Adversarial Nets\n\n\nauthor : Goodfellow et al.\n\n\nyear : 2014\n\nidea\n\n\n두 신경망(Generator 와 Discriminator)을 경쟁적으로 학습시켜 실제와 구별하기 어려운 가짜 데이터를 생성하는 프레임 워크.\n\n\n구조\n\n\nGenerator : 노이즈 벡터 z를 입력받아 가짜 데이터 생성\nDiscriminator : 실제 데이터와 생성된 데이터를 구별\n\n\n메커니즘\n\n\nMinimax Game : \\(min_G\\;max_D\\;=\\;E[logD(x)]\\;+\\;E[log(1-D(G(z)))]\\)\nD는 실제 데이터에 높은 확률, 가짜 데이터에 낮은 확률 할당하도록 학습\nG는 D가 구별하지 못하도록 더 실제같은 데이터 생성 학습\n\n\n세부사항\n\n\n명시적 확률 밀도 함수 없이도 생성 모델 학습 가능\n역전파를 통한 end-to-end 학습\nNash 균형점에서 이론적으로 최적해 달성\n\n\n\n1. imports\n\nimport torch \nimport torchvision\nimport matplotlib.pyplot as plt \n\n\n\n2. GAN(Goodfellow et al. 2014) intro\n\nA. 생성모형이란?\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼 수 있는가?\n- 진정으로 인공지능이 이미지자료를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공\n- 뭘 분류하려는 목적을 가진게 판별모형이면 뭘 만들려는 목적을 가진게 생성모형이고 생성모형이 더 우수하다.\n\n명언: 만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n\n\n\n3. GAN의 구현\n\nA. Data\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=False)\nto_tensor = torchvision.transforms.ToTensor()\nX_real = torch.stack([to_tensor(Xi) for Xi, yi in train_dataset if yi==3])\n\n\nplt.imshow(X_real[0].squeeze(),cmap=\"gray\")\n\n\n\n\n\n\n\n\n\n\nB. 페이커 생성\n- net_faker : noise \\(\\to\\) 가짜이미지”를 만들자\n- 네트워크의 입력 : (n,??)인 랜덤으로 뽑은 숫자\n\ntorch.randn(1,4) # 이게 입력으로 온다고 상상하자. \n\ntensor([[ 0.3833,  1.4574,  0.6266, -0.1444]])\n\n\n- 네트워크의 출력: (n,1,28,28)의 텐서\n\nclass FlattenToImage(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,X):\n        return X.reshape(-1,1,28,28)\nnet_facker = torch.nn.Sequential(\n    torch.nn.Linear(4,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,784),\n    torch.nn.Sigmoid(), # 출력을 0~1로 눌러주기 위한 레이어 // 저한테는 일종의 문화충격\n    FlattenToImage()\n)\n\n\nnet_facker(torch.randn(1,4)).shape\n\ntorch.Size([1, 1, 28, 28])\n\n\n\n\nC. 경찰 생성\n- net_police : 진짜 이미지 \\(\\to\\) 0 , 가짜 이미지 \\(\\to\\) 1 과 같은 네트워크 설계\n- 네트워크의 입력 : (n,1,28,28) 인 이미지\n- 네트워크의 출력 : 0, 1\n\nnet_police = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(30,1),\n    torch.nn.Sigmoid()\n)\n\n\n\nD. 바보 경찰과 바보 페이커\n- 데이터\n\nreal_image = X_real[[0]]  # 진짜이미지\nfake_image = net_facker(torch.randn(1,4)).data # 가짜이미지\n\n- 경찰 네트워크가 가짜 이미지와, 진짜 이미지를 봤을 때 각각 어떤 판단을 할까\n-진짜 이미지를 봤을 때\n\nnet_police(real_image) # -&gt; 0으로 가야함\n\ntensor([[0.4829]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n- 가짜 이미지를 봤을 떄\n\nnet_police(fake_image) # -&gt; 1로 가야함\n\ntensor([[0.4764]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n- 아직 아쉬운 판단..\n\n\nE. 똑똑해진 경찰\n- 데이터를 정리\n\n원래 \\(n=6131\\)개의 이미지자료가 있었음. 이를 \\({\\bf X}_{real}\\) 로 저장했었음.\n\\({\\bf X}_{fake}\\)는 net_facker의 output으로 생성하고 꼬리표 제거.\n\\({\\bf X}_{real}\\)에 대응하는 \\({\\bf y}_{real}\\) 생성. 진짜이미지는 라벨을 0으로 정함.\n\\({\\bf X}_{faker}\\)에 대응하는 \\({\\bf y}_{fake}\\) 생성. 가짜이미지는 라벨을 1로 정함.\n\n\nX_fake = net_facker(torch.randn(6131,4)).data\ny_real = torch.zeros((6131,1))\ny_fake = torch.ones((6131,1))\n\n- step1: X_real, X_fake를 보고 각각 yhat_real, yhat_fake를 만드는 과정\n\nyhat_real = net_police(X_real)\nyhat_fake = net_police(X_fake)\n\n- step2: 경찰의 미덕은 (1) 가짜이미지를 가짜라고 하고 (2) 진짜이미지를 진짜라고 해야함.\n- 즉 yhat_real 은 거의 0의 값으로, 그리고 yhat_fake는 1이 되도록 설계해야함. (yhat_real \\(\\approx\\) y_real 이고 yhat_fake \\(\\approx\\) y_fake 이어야 함) 이러면 경찰이 잘하는것.\n\nbce = torch.nn.BCELoss()\n\n\nloss_police = bce(yhat_real,y_real) + bce(yhat_fake,y_fake) \nloss_police\n\ntensor(1.3918, grad_fn=&lt;AddBackward0&gt;)\n\n\n- 합쳐서 계산하는 방법\n\ntorch.concat([X_real,X_fake],axis=0).shape\n\ntorch.Size([12262, 1, 28, 28])\n\n\n\ntorch.concat([y_real,y_fake],axis=0).shape\n\ntorch.Size([12262, 1])\n\n\n\nbce(net_police(torch.concat([X_real,X_fake],axis=0)),torch.concat([y_real,y_fake],axis=0))*2\n\ntensor(1.3918, grad_fn=&lt;MulBackward0&gt;)\n\n\n-step 3~4\n\n# net_police = torch.nn.Sequential(\n#     torch.nn.Flatten(),\n#     torch.nn.Linear(784,30),\n#     torch.nn.ReLU(),\n#     torch.nn.Linear(30,1),\n#     torch.nn.Sigmoid()\n# )\nbce = torch.nn.BCELoss()\noptimizr_police = torch.optim.Adam(net_police.parameters())\nfor epoc in range(30):\n    X_fake = net_facker(torch.randn(6131,4)).data\n    # step1 -- yhat을 얻음\n    yhat_real = net_police(X_real)\n    yhat_fake = net_police(X_fake)\n    # step2  -- loss를 계산\n    loss_police = bce(yhat_real,y_real) + bce(yhat_fake,y_fake)\n    # step3  -- 미분 \n    loss_police.backward()\n    # step4 -- update \n    optimizr_police.step()\n    optimizr_police.zero_grad()\n\n- 경찰의 실력향상 감상\n\nnet_police(X_real) # 거의 0으로 \n\ntensor([[0.0099],\n        [0.0126],\n        [0.0132],\n        ...,\n        [0.0165],\n        [0.1010],\n        [0.0247]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet_police(net_facker(torch.randn(6131,4)).data) # 거의 1로\n\ntensor([[0.9775],\n        [0.9775],\n        [0.9773],\n        ...,\n        [0.9773],\n        [0.9773],\n        [0.9775]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n- 꽤 우수한 경찰..\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(X_fake[[2]].squeeze(),cmap=\"gray\")\nax[0].set_title(f\"police output = {net_police(X_fake[[2]]).item():.4f}\")\nax[1].imshow(X_real[[-1]].squeeze(),cmap=\"gray\")\nax[1].set_title(f\"police output = {net_police(X_real[[-1]]).item():.4f}\")\n\nText(0.5, 1.0, 'police output = 0.0247')\n\n\n\n\n\n\n\n\n\n\n\nF. 더 똑똑해지는 페이커\n- step1 : noise \\(\\to\\) X_fake\n\nX_fake = net_facker(torch.randn(6131,4))\n# 여기서는 X_fake가 데이터가 아니고 네트워크 출력이므로 꼬리표를 제거하지 말아야함\n\n- step2: 손실함수 - 페이커의 미덕 (잘 훈련된) 경찰이 가짜이미지를 진짜라고 판단하는 것. 즉 yhat_fake \\(\\approx\\) y_real 이어야 페이커의 실력이 우수하다고 볼 수 있음.\n\nyhat_fake = net_police(X_fake)\nloss_faker = bce(yhat_fake, y_real) \n# 가짜이미지를 보고 잘 훈련된 경찰도  \n# 진짜 이미지라고 깜빡 속으면 \n# 위조범의 실력이 좋다고 볼 수 있다는 의미\n\n\nclass FlattenToImage(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,X):\n        return X.reshape(-1,1,28,28)\nnet_facker = torch.nn.Sequential(\n    torch.nn.Linear(4,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,784),\n    torch.nn.Sigmoid(), # 출력을 0~1로 눌러주기 위한 레이어 // 저한테는 일종의 문화충격\n    FlattenToImage()\n)\nbce = torch.nn.BCELoss()\noptimizr_facker = torch.optim.Adam(net_facker.parameters())\n\n\nfor epoc in range(1):\n    # step1 -- yhat을 얻음\n    X_fake = net_facker(torch.randn(6131,4))\n    # step2  -- loss를 계산\n    yhat_fake = net_police(X_fake)\n    loss_faker = bce(yhat_fake,y_real)\n    # step3  -- 미분 \n    loss_faker.backward()\n    # step4 -- update \n    optimizr_facker.step()\n    optimizr_facker.zero_grad()\n\n- 위조범의 실력향상 감상\n\nplt.imshow(X_fake[[0]].squeeze().data,cmap=\"gray\")\n\n\n\n\n\n\n\n\n\n\nG. 경쟁학습 (최종코드)\n\ntorch.manual_seed(43052)\nnet_police = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(30,1),\n    torch.nn.Sigmoid()\n)\nclass FlattenToImage(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,X):\n        return X.reshape(-1,1,28,28)\nnet_facker = torch.nn.Sequential(\n    torch.nn.Linear(4,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,784),\n    torch.nn.Sigmoid(), # 출력을 0~1로 눌러주기 위한 레이어 // 저한테는 일종의 문화충격\n    FlattenToImage()\n)\nbce = torch.nn.BCELoss()\noptimizr_police = torch.optim.Adam(net_police.parameters(),lr=0.001, betas=(0.5,0.999))\noptimizr_facker = torch.optim.Adam(net_facker.parameters(),lr=0.0002, betas=(0.5,0.999))\n\n\nfor epoc in range(1000):\n    #--- net_police 를 훈련 \n    #step1\n    X_fake = net_facker(torch.randn(6131,4)).data # 여기에서 X_fake는 data를 의미\n    yhat_real = net_police(X_real)\n    yhat_fake = net_police(X_fake)\n    #step2\n    loss_police = bce(yhat_real,y_real) + bce(yhat_fake,y_fake)\n    #step3\n    loss_police.backward()\n    #step4\n    optimizr_police.step()\n    optimizr_police.zero_grad()\n    #--- net_faker 를 훈련 \n    #step1\n    X_fake = net_facker(torch.randn(6131,4)) # 이때 X_fake는 net의 out을 의미 \n    #step2\n    yhat_fake = net_police(X_fake)\n    loss_facker = bce(yhat_fake, y_real)\n    #step3\n    loss_facker.backward()\n    #step4\n    optimizr_facker.step()\n    optimizr_facker.zero_grad()\n\n\nfig, ax  = plt.subplots(2,5,figsize=(10,4))\nk=0\nfor i in range(2):\n    for j in range(5):\n        ax[i][j].imshow(X_fake[[k]].data.squeeze(),cmap=\"gray\")\n        ax[i][j].set_title(f\"police out = {net_police(X_fake[[k]]).item():.4f}\")\n        k= k+1\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n5. 초기 GAN의 한계점\n- 두 네트워크의 균형이 매우 중요함 – 균형이 깨지는 순간 학습은 실패함\n- 생성되는 이미지의 다양성이 부족한 경우가 발생함. (mode collapse)\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” Advances in Neural Information Processing Systems 27.\n- 적당히 비슷해야함\n\n경찰이 너무 똑똑하면 (판별을 잘하면)..학습을 다 못해버림\n아니면 속이는 다 똑같은 이미지 생성"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "paper_code",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 17, 2025\n\n\n1. Class Activation Maps(CAM)\n\n\n이상민 \n\n\n\n\nOct 17, 2025\n\n\n2. Generative Adversarial Network(GAN))\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01_Class_Actiation_Map.html",
    "href": "posts/01_Class_Actiation_Map.html",
    "title": "1. Class Activation Maps(CAM)",
    "section": "",
    "text": "title : Learning Deep Features for Discriminative Localization\n\n\nauthor : Zhou et al.\n\n\nyear : 2016\n\nidea\n\n\nCNN이 이미지 분류 시 어떤 영역에 주목하는지 시각화하는 기법. 별도의 localization 학습 없이도 분류 모델이 객체의 위치를 파악할 수 있음을 보여줌.\n\n\n메커니즘\n\n\nGlobal Average Pooling(GAP)를 사용하여 각 feature map의 공간 정보를 보존\n마지막 convolutional layer의 feature maps에 분류 가중치를 적용\n가중합을 통해 클래스별 활성화 맵 생성\n\n\n세부사항\n\n\n기존 fully connected layer를 GAP + 단일 linear layer로 대체\n각 feature map에 해당하는 가중치가 특정 클래스에 대한 중요도를 나타냄\n\\(CAM \\; = \\; \\sum(weight_k \\; \\times \\;feature \\; map_k)\\)\n\n\n\n1. imports\n\nimport torch\nimport torchvision\nimport PIL\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n\n\n\n2. 이미지 자료 처리\n\nA. 데이터\n- 데이터 다운로드\n\ntorchvision.__version__\n\n'0.20.1'\n\n\n\ntrain_dataset = torchvision.datasets.OxfordIIITPet(\n    root='./data', \n    split='trainval',\n    download=True,\n    target_types='binary-category'\n)\ntest_dataset = torchvision.datasets.OxfordIIITPet(\n    root='./data', \n    split='test',\n    download=True,\n    target_types='binary-category'\n)\n\n- 고양이는 0, 강아지는 1\n\ntrain_dataset[0][0]\n\n\n\n\n\n\n\n\n\ntrain_dataset[0][1]\n\n0\n\n\n\n\nB. 이미지 변환\n- x_pil 을 tensor로 바꾸어 보자\n\nx_pil = train_dataset[0][0]\nx_pil\n\n\n\n\n\n\n\n\n\nto_tensor = torchvision.transforms.ToTensor() # PIL를 텐서로 만드는 함수를 리턴\nx_tensor=to_tensor(x_pil)\nprint(x_tensor.shape)\nx_tensor\n\ntorch.Size([3, 500, 394])\n\n\ntensor([[[0.1451, 0.1373, 0.1412,  ..., 0.9686, 0.9765, 0.9765],\n         [0.1373, 0.1373, 0.1451,  ..., 0.9647, 0.9725, 0.9765],\n         [0.1373, 0.1412, 0.1529,  ..., 0.9686, 0.9804, 0.9804],\n         ...,\n         [0.0196, 0.0157, 0.0157,  ..., 0.2863, 0.2471, 0.2706],\n         [0.0157, 0.0118, 0.0118,  ..., 0.2392, 0.2157, 0.2510],\n         [0.1098, 0.1098, 0.1059,  ..., 0.2314, 0.2549, 0.2980]],\n\n        [[0.0784, 0.0706, 0.0745,  ..., 0.9725, 0.9725, 0.9725],\n         [0.0706, 0.0706, 0.0784,  ..., 0.9686, 0.9686, 0.9725],\n         [0.0706, 0.0745, 0.0863,  ..., 0.9647, 0.9765, 0.9765],\n         ...,\n         [0.0235, 0.0196, 0.0196,  ..., 0.4627, 0.4039, 0.4314],\n         [0.0118, 0.0078, 0.0078,  ..., 0.3882, 0.3843, 0.4235],\n         [0.1059, 0.1059, 0.1059,  ..., 0.3686, 0.4157, 0.4588]],\n\n        [[0.0471, 0.0392, 0.0431,  ..., 0.9922, 0.9922, 0.9922],\n         [0.0392, 0.0392, 0.0471,  ..., 0.9843, 0.9882, 0.9922],\n         [0.0392, 0.0431, 0.0549,  ..., 0.9843, 0.9961, 0.9961],\n         ...,\n         [0.0941, 0.0902, 0.0902,  ..., 0.9608, 0.9216, 0.8784],\n         [0.0745, 0.0706, 0.0706,  ..., 0.9255, 0.9373, 0.8980],\n         [0.1373, 0.1373, 0.1373,  ..., 0.8392, 0.9098, 0.8745]]])\n\n\n\nplt.imshow(to_tensor(x_pil) .permute(1,2,0))\n\n\n\n\n\n\n\n\n- 궁극적으로는 train_dataset의 모든 이미지를 (3680,3,???,???)로 정리하여 X라고 하고 싶음. \\(\\to\\) 이걸 하기 위해서는 이미지 크기를 통일시켜야함. \\(\\to\\) 이미지크기를 통일시키는 방법을 알아보자.\n\nto_tensor(train_dataset[0][0]).shape\n\ntorch.Size([3, 500, 394])\n\n\n\nto_tensor(train_dataset[1][0]).shape\n\ntorch.Size([3, 313, 450])\n\n\n- 512,512로 이미지 조정\n\nresize = torchvision.transforms.Resize((512,512)) # 512,512로 이미지를 조정해주는 함수가 리턴\n\n\nto_tensor(resize(train_dataset[0][0])).shape\n\ntorch.Size([3, 512, 512])\n\n\n\nplt.imshow(to_tensor(resize(train_dataset[0][0])).permute(1,2,0))\n\n\n\n\n\n\n\n\n- 크기가 8인 이미지들의 배치를 만들기\n\nXm = torch.stack([to_tensor(resize(train_dataset[n][0])) for n in range(8)],axis=0)\nXm.shape\n\ntorch.Size([8, 3, 512, 512])\n\n\n\n\n5. AP Layer\n- 채널별로 평균을 구하는 Layer\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1)\n\n\nX = torch.arange(1*3*4*4).reshape(1,3,4,4).float()\nX\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(X) # 채널별평균\n\ntensor([[[[ 7.5000]],\n\n         [[23.5000]],\n\n         [[39.5000]]]])\n\n\n\n\nB.AP, Linear의 교환\n\nr = X[:,0,:,:]\ng = X[:,1,:,:]\nb = X[:,2,:,:]\n\n\nap(r)*0.1 + ap(g)*0.2 + ap(b)*0.3\n\ntensor([[[17.3000]]])\n\n\n\nap(r*0.1 + g*0.2 + b*0.3)\n\ntensor([[[17.3000]]])\n\n\n- 위의 두 계산결과를 torch.nn.AdaptiveAvgPool2d, torch.nn.Linear, 그리고 torch.nn.Flatten()을 조합하여 구현해보자.\n\n# ap(r)*0.1 + ap(g)*0.2 + ap(b)*0.3\nap = torch.nn.AdaptiveAvgPool2d(output_size=1)\nflattn = torch.nn.Flatten()\nlinr = torch.nn.Linear(3,1,bias=False)\nlinr.weight.data = torch.tensor([[ 0.1,  0.2, 0.3]])\n#---#\nprint(X.shape)\nprint(ap(X).shape) # ap(r), ap(g), ap(b) 의 값이 들어있음.\nprint(flattn(ap(X)).shape) # [ap(r), ap(g), ap(b)] 형태로\nprint(linr(flattn(ap(X))).shape) # ap(r)*0.1 + ap(g)*0.2 + ap(b)*0.3\n\ntorch.Size([1, 3, 4, 4])\ntorch.Size([1, 3, 1, 1])\ntorch.Size([1, 3])\ntorch.Size([1, 1])\n\n\n\n#ap(r*0.1 + g*0.2 + b*0.3)\nap = torch.nn.AdaptiveAvgPool2d(output_size=1)\nflattn = torch.nn.Flatten()\nlinr = torch.nn.Linear(3,1,bias=False)\nlinr.weight.data = torch.tensor([[ 0.1,  0.2, 0.3]])\n#---#\ndef _linr(X):\n    return torch.einsum('ocij, kc -&gt; okij', X, linr.weight.data)\n#---#\nprint(X.shape) # \nprint(_linr(X).shape) # r*0.1 + g*0.2 + b*0.3 \nprint(ap(_linr(X)).shape) # ap(r*0.1 + g*0.2 + b*0.3 )\nprint(flattn(ap(_linr(X))).shape)\n\ntorch.Size([1, 3, 4, 4])\ntorch.Size([1, 1, 4, 4])\ntorch.Size([1, 1, 1, 1])\ntorch.Size([1, 1])\n\n\n\n\n\n5. CAM(Ahou et al. 2016)의 구현\n\nA. 0단계 – (X,y), (XX,yy)\n\ntrain_dataset = torchvision.datasets.OxfordIIITPet(\n    root='./data', \n    split='trainval',\n    download=True,\n    target_types='binary-category',\n)\ntest_dataset = torchvision.datasets.OxfordIIITPet(\n    root='./data', \n    split='test',\n    download=True,\n    target_types='binary-category',\n)\n\n\ncompose = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((512,512)),\n    torchvision.transforms.ToTensor()\n])\n\n\nX = torch.stack([compose(train_dataset[i][0]) for i in range(3680)],axis=0)\nXX = torch.stack([compose(test_dataset[i][0]) for i in range(3669)],axis=0)\ny = torch.tensor([train_dataset[i][1] for i in range(3680)]).reshape(-1,1).float()\nyy = torch.tensor([test_dataset[i][1] for i in range(3669)]).reshape(-1,1).float()\n\n\n\nB. 1단계 - 이미지 분류 잘하는 네트워크 선택 후 학습\n\ntorch.manual_seed(43052)\n#--Step1\nds_train = torch.utils.data.TensorDataset(X,y)\ndl_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\nds_test = torch.utils.data.TensorDataset(XX,yy)\ndl_test = torch.utils.data.DataLoader(ds_test, batch_size=32)\n#--Step2\nresnet18 = torchvision.models.resnet18(pretrained=True)\nresnet18.fc = torch.nn.Linear(512,1)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(resnet18.parameters(), lr=1e-5)\n#--Step3 \nresnet18.to(\"cuda:0\")\nfor epoc in range(3):\n    resnet18.train()\n    for Xm,ym in dl_train:\n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        #1\n        netout = resnet18(Xm)\n        #2\n        loss = loss_fn(netout,ym)\n        #3\n        loss.backward()\n        #4\n        optimizr.step()\n        optimizr.zero_grad()\n    #---# \n    resnet18.eval()\n    s = 0 \n    for Xm,ym in dl_train:\n        Xm = Xm.to(\"cuda:0\")\n        ym = ym.to(\"cuda:0\")\n        s = s + ((resnet18(Xm).data &gt; 0) == ym).sum().item()\n    acc = s/3680\n    print(f\"train_acc = {acc:.4f}\")\n#--Step4 \nresnet18.eval()\ns = 0 \nfor Xm,ym in dl_test:\n    Xm = Xm.to(\"cuda:0\")\n    ym = ym.to(\"cuda:0\")\n    s = s + ((resnet18(Xm).data &gt; 0) == ym).sum().item()\nacc = s/3669\nprint(f\"test_acc = {acc:.4f}\")\n\n\n\nC. 2단계 - Linear 와 AP의 순서를 바꿈\n- resnet18을 재구성하여 net을 만들자\n\nstem = torch.nn.Sequential(\n    torch.nn.Sequential(\n        resnet18.conv1,\n        resnet18.bn1,\n        resnet18.relu,\n        resnet18.maxpool\n    ),\n    resnet18.layer1,\n    resnet18.layer2,\n    resnet18.layer3,\n    resnet18.layer4\n)\nhead = torch.nn.Sequential(\n    resnet18.avgpool,\n    torch.nn.Flatten(),\n    resnet18.fc\n)\nnet = torch.nn.Sequential(\n    stem,\n    head\n)\n\n- 아직은 resnet18과 똑같은 기능, 인덱스로 접근 가능!\n- 1개의 observation을 고정\n\nx = X[[0]].to(\"cuda:0\")\n\n- 이렇게 해도 될듯\n\ntorch.stack([X[0]],axis=0).shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\nnet(x), resnet18(x)\n\n(tensor([[-5.5534]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[-5.5534]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;))\n\n\n- 위와 같은 값 -5.5534이 나오는 과정을 추적하여 보자.\n\n# 계산방식1: 원래계산방식\nap = head[0]\nflattn = head[1]\nlinr = head[2]\n#---#\nprint(f\"{x.shape} -- x\")\nprint(f\"{stem(x).shape} -- stem(x)\")\nprint(f\"{ap(stem(x)).shape} -- ap(stem(x))\")\nprint(f\"{flattn(ap(stem(x))).shape} -- flattn(ap(stem(x)))\")\nprint(f\"{linr(flattn(ap(stem(x)))).shape} -- linr(flattn(ap(stem(x))))\")\n\ntorch.Size([1, 3, 512, 512]) -- x\ntorch.Size([1, 512, 16, 16]) -- stem(x)\ntorch.Size([1, 512, 1, 1]) -- ap(stem(x))\ntorch.Size([1, 512]) -- flattn(ap(stem(x)))\ntorch.Size([1, 1]) -- linr(flattn(ap(stem(x))))\n\n\n현재 네트워크 \\[\\underset{(1,3,512,512)}{\\boldsymbol x} \\overset{stem}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{ap}{\\to} \\underset{(1,512,1,1)}{{\\boldsymbol \\sharp}}\\overset{flattn}{\\to} \\underset{(1,512)}{{\\boldsymbol \\sharp}}\\overset{linr}{\\to} \\underset{(1,1)}{logit}\\right) = [[-5.5613]]\\]\n바꾸고 싶은 네트워크 \\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{stem}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{\\_linr}{\\to} \\underset{(1,1,16,16)}{{\\boldsymbol \\sharp}}\\overset{ap}{\\to} \\underset{(1,1,1,1)}{{\\boldsymbol \\sharp}}\\overset{flattn}{\\to} \\underset{(1,1)}{logit}\\right) = [[-5.5613]]\\]\n\n# 계산방식2\nap = head[0]\nflattn = head[1]\nlinr = head[2]\ndef _linr(xtilde):\n    return torch.einsum('ocij, kc -&gt; okij', xtilde, linr.weight.data) + linr.bias.data\n#---#\nprint(f\"{x.shape} -- x\")\nprint(f\"{stem(x).shape} -- stem(x)\")\nprint(f\"{_linr(stem(x)).shape} -- _linr(stem(x))\")\nprint(f\"{ap(_linr(stem(x))).shape} -- ap(_linr(stem(x)))\")\nprint(f\"{flattn(ap(_linr(stem(x)))).shape} -- flattn(ap(_linr(stem(x))))\")\n\ntorch.Size([1, 3, 512, 512]) -- x\ntorch.Size([1, 512, 16, 16]) -- stem(x)\ntorch.Size([1, 1, 16, 16]) -- _linr(stem(x))\ntorch.Size([1, 1, 1, 1]) -- ap(_linr(stem(x)))\ntorch.Size([1, 1]) -- flattn(ap(_linr(stem(x))))\n\n\n\nlinr(flattn(ap(stem(x)))), flattn(ap(_linr(stem(x))))\n\n(tensor([[-5.5534]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[-5.5534]], device='cuda:0', grad_fn=&lt;ViewBackward0&gt;))\n\n\n- 원래 계산방식을 적용\n\nlinr(flattn(ap(stem(x))))\n\ntensor([[-5.5534]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\n\n\n- 바뀐 계산방식을 적용\n\nflattn(ap(_linr(stem(x))))\n\ntensor([[-5.5534]], device='cuda:0', grad_fn=&lt;ViewBackward0&gt;)\n\n\n- 바뀐 계산방식을 좀더 파고 들어서 분석해보자.\n\n_linr(stem(x)).long()\n\ntensor([[[[  0,   0,   0,   0,   0,   0,   0,   0,  -1,  -2,  -2,  -2,   0,   0,\n             0,   0],\n          [  0,   0,   0,   0,   0,   0,   0,  -1,   0,  -2,  -3,  -4,  -1,   0,\n             0,   0],\n          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  -3,  -5,  -4,  -2,\n             0,   0],\n          [  0,   0,   0,   0,   0,   0,   0,   0,  -1,  -4,  -9, -12, -12,  -8,\n            -2,   0],\n          [  0,   0,   0,   0,   0,   0,  -1,  -5, -11, -16, -20, -24, -22, -14,\n            -4,   0],\n          [ -1,  -1,   0,   0,   0,   0,  -5, -16, -28, -35, -40, -42, -37, -23,\n            -7,   0],\n          [ -1,  -1,   0,   0,   0,   0, -10, -28, -47, -56, -56, -52, -42, -25,\n            -7,   0],\n          [  0,  -1,   0,   0,   0,   0, -11, -29, -49, -57, -54, -47, -34, -19,\n            -4,   1],\n          [  0,   0,   0,   0,   0,   0,  -8, -21, -36, -42, -38, -29, -18,  -8,\n            -1,   0],\n          [  0,   0,  -1,  -1,   0,   0,  -3,  -9, -16, -19, -16, -11,  -4,   0,\n             0,   0],\n          [  0,   0,  -2,  -2,   0,   0,  -1,  -3,  -6,  -6,  -5,  -2,   0,   0,\n             1,   1],\n          [  0,   0,  -2,  -1,   0,   0,  -1,  -3,  -4,  -3,  -1,   0,   0,   0,\n             1,   1],\n          [  0,   0,  -1,   0,   0,   0,   0,  -2,  -2,  -1,   0,   0,   0,   0,\n             1,   1],\n          [  0,   0,   0,   0,   0,   0,   0,  -1,  -1,   0,   0,   0,   0,   1,\n             2,   1],\n          [ -1,  -1,  -1,  -1,   0,   0,   0,  -1,   0,   0,   0,   1,   1,   2,\n             2,   2],\n          [  0,  -1,  -1,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,\n             1,   1]]]], device='cuda:0')\n\n\n\n여러가지 값들이 있지만 (16*16=256개의 값들) 아무튼 이 값들의 평균은 -5.5613 임. (이 값이 작을수록 이 그림은 고양이라는 의미임)\n그런데 살펴보니까 대부분의 위치에서 -에 가까운 값을가지고, 특정위치에서만 엄청 작은 값이 존재하여 -5.5613 이라는 평균값이 나오는 것임.\n결국 이 특정위치에 존재하는 엄청 작은 값들이 x가 고양이라고 판단하는 근거가 된다.\n\n바꾸고 싶은 네트워크 – why라는 이름을 적용하여 \\[\\underset{(1,3,224,224)}{\\boldsymbol x} \\overset{stem}{\\to} \\left( \\underset{(1,512,16,16)}{\\tilde{\\boldsymbol x}} \\overset{\\_linr}{\\to} \\underset{(1,1,16,16)}{\\bf why}\\overset{ap}{\\to} \\underset{(1,1,1,1)}{{\\boldsymbol \\sharp}}\\overset{flattn}{\\to} \\underset{(1,1)}{logit}\\right) = [[-5.5613]]\\]\n\nwhy = _linr(stem(x)) \n\n\n\nD. 3단계 -WHY 시각화\n- 시각화1 - why와 img를 겹쳐서 그려보자\n\nplt.imshow(why.squeeze().cpu().detach(),cmap=\"bwr\")\nplt.colorbar()\n\n\n\n\n\n\n\n\n\nplt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\nwhy_resized = torch.nn.functional.interpolate(\n    why,\n    size=(512,512),\n    mode=\"bilinear\",\n)\nplt.imshow(why_resized.squeeze().cpu().detach(),cmap=\"bwr\",alpha=0.5)\nplt.colorbar()\n\n\n\n\n\n\n\n\n- 시각화2 - colormap을 magma로 적용\n\nx = X[[0]].to(\"cuda:0\")\nif net(x) &gt; 0: \n    pred = \"dog\"\n    why = _linr(stem(x)) \nelse: \n    pred = \"cat\" \n    why = - _linr(stem(x)) \nplt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\nwhy_resized = torch.nn.functional.interpolate(\n    why,\n    size=(512,512),\n    mode=\"bilinear\",\n)\nplt.imshow(why_resized.squeeze().cpu().detach(),cmap=\"magma\",alpha=0.5)\nplt.title(f\"prediction = {pred}\");\n\n\n\n\n\n\n\n\n- 시각화3 - 하니를 시각화\n\nurl = 'https://github.com/guebin/DL2025/blob/main/imgs/hani1.jpeg?raw=true'\nhani_pil = PIL.Image.open(\n    io.BytesIO(requests.get(url).content)\n)\n\n\nx = compose(hani_pil).reshape(1,3,512,512).to(\"cuda:0\")\nif net(x) &gt; 0: \n    pred = \"dog\"\n    why = _linr(stem(x)) \nelse: \n    pred = \"cat\" \n    why = - _linr(stem(x)) \nplt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\nwhy_resized = torch.nn.functional.interpolate(\n    why,\n    size=(512,512),\n    mode=\"bilinear\",\n)\nplt.imshow(why_resized.squeeze().cpu().detach(),cmap=\"magma\",alpha=0.5)\nplt.title(f\"prediction = {pred}\");\n\n\n\n\n\n\n\n\n- 시각화4 - XX의 이미지를 시각화해보자\n\nfig, ax = plt.subplots(5,5)\n#---#\nk = 0\nfor i in range(5):\n    for j in range(5):\n        x = XX[[k]].to(\"cuda:0\")\n        if net(x) &gt; 0: \n            pred = \"dog\"\n            why = _linr(stem(x)) \n        else: \n            pred = \"cat\" \n            why = - _linr(stem(x)) \n        plt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\n        why_resized = torch.nn.functional.interpolate(\n            why,\n            size=(512,512),\n            mode=\"bilinear\",\n        )\n        ax[i][j].imshow(x.squeeze().permute(1,2,0).cpu())\n        ax[i][j].imshow(why_resized.squeeze().cpu().detach(),cmap=\"magma\",alpha=0.5)\n        ax[i][j].set_title(f\"prediction = {pred}\");\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n        k = k+50\nfig.set_figheight(16)\nfig.set_figwidth(16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5)\n#---#\nk = 1 \nfor i in range(5):\n    for j in range(5):\n        x = XX[[k]].to(\"cuda:0\")\n        if net(x) &gt; 0: \n            pred = \"dog\"\n            why = _linr(stem(x)) \n        else: \n            pred = \"cat\" \n            why = - _linr(stem(x)) \n        plt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\n        why_resized = torch.nn.functional.interpolate(\n            why,\n            size=(512,512),\n            mode=\"bilinear\",\n        )\n        ax[i][j].imshow(x.squeeze().permute(1,2,0).cpu())\n        ax[i][j].imshow(why_resized.squeeze().cpu().detach(),cmap=\"magma\",alpha=0.5)\n        ax[i][j].set_title(f\"prediction = {pred}\");\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n        k = k+50\nfig.set_figheight(16)\nfig.set_figwidth(16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5)\n#---#\nk = 2\nfor i in range(5):\n    for j in range(5):\n        x = XX[[k]].to(\"cuda:0\")\n        if net(x) &gt; 0: \n            pred = \"dog\"\n            why = _linr(stem(x)) \n        else: \n            pred = \"cat\" \n            why = - _linr(stem(x)) \n        plt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\n        why_resized = torch.nn.functional.interpolate(\n            why,\n            size=(512,512),\n            mode=\"bilinear\",\n        )\n        ax[i][j].imshow(x.squeeze().permute(1,2,0).cpu())\n        ax[i][j].imshow(why_resized.squeeze().cpu().detach(),cmap=\"magma\",alpha=0.5)\n        ax[i][j].set_title(f\"prediction = {pred}\");\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n        k = k+50\nfig.set_figheight(16)\nfig.set_figwidth(16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(5,5)\n#---#\nk = 3\nfor i in range(5):\n    for j in range(5):\n        x = XX[[k]].to(\"cuda:0\")\n        if net(x) &gt; 0: \n            pred = \"dog\"\n            why = _linr(stem(x)) \n        else: \n            pred = \"cat\" \n            why = - _linr(stem(x)) \n        plt.imshow(x.cpu().detach().squeeze().permute(1,2,0))\n        why_resized = torch.nn.functional.interpolate(\n            why,\n            size=(512,512),\n            mode=\"bilinear\",\n        )\n        ax[i][j].imshow(x.squeeze().permute(1,2,0).cpu())\n        ax[i][j].imshow(why_resized.squeeze().cpu().detach(),cmap=\"magma\",alpha=0.5)\n        ax[i][j].set_title(f\"prediction = {pred}\");\n        ax[i][j].set_xticks([])\n        ax[i][j].set_yticks([])\n        k = k+50\nfig.set_figheight(16)\nfig.set_figwidth(16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n6. CAM의 한계\n- 구조적 제약이 있음\n\nhead-part가 ap와 linr로만 구성되어야 가능\n그렇지 않은 네트워크는 임의로 재구성하여 head-part를 ap와 linr로많 구성해야함(CAM을 적용하기 위한 구조로 만들기 위해)\n\n- 이후로 등장한 grad-cam은 구조의 제약 없이 거의 모든 CNN에 적용 가능"
  }
]